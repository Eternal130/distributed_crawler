Distributed web crawlers achieve the following functions:
1. Analyze the URLs contained in a given URL and crawl the corresponding webpage until all pages are crawled
Until there are no duplicate web pages.
2. Support distributed crawling while recording and outputting the size of each webpage.
3. Adopting a multi-threaded architecture design to achieve high-performance web crawlers.
